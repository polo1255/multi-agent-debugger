# file: server.py
from fastapi.responses import StreamingResponse
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from graph.workflow import app
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import uuid
import os
import json
import asyncio
from dotenv import load_dotenv
from database.vector_store import search_similar_bugs, save_bug_report

# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env
load_dotenv()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Server
api = FastAPI(title="Multi-Agent Debugger API")

class DebugRequest(BaseModel):
    code: str
    error: str

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ AI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á DeepSeek ‡πÅ‡∏•‡∏∞ OpenAI) ---
summarizer_llm = None

if os.getenv("DEEPSEEK_API_KEY"):
    # ‡πÉ‡∏ä‡πâ DeepSeek (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î)
    summarizer_llm = ChatOpenAI(
        model="deepseek-coder", 
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0.5
    )
    print("‚úÖ Summarizer Agent: Using DeepSeek")
elif os.getenv("OPENAI_API_KEY"):
    # ‡πÉ‡∏ä‡πâ OpenAI
    summarizer_llm = ChatOpenAI(
        model="gpt-3.5-turbo", 
        api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0.5
    )
    print("‚úÖ Summarizer Agent: Using OpenAI")
else:
    print("‚ö†Ô∏è Warning: No API Key found for Summarizer. Summary feature will be disabled.")

@api.get("/")
def read_root():
    return {"status": "Agent System is Ready!"}

@api.post("/debug")
async def debug_code(request: DebugRequest): # 1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô async def
    print(f"--- RECEIVING REQUEST (STREAMING MODE) ---")

    # ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ
    print("üß† Searching Vector Store...")
    similar_cases = search_similar_bugs(request.error, request.code)
    
    knowledge_str = ""
    if similar_cases:
        knowledge_str = "\n".join([
            f"--- Reference Case ---\n{c['summary']}" 
            for c in similar_cases
        ])

    initial_state = {
        "code_base": request.code,
        "error_context": request.error,
        "knowledge_context": knowledge_str,
        "reflection_logs": [],
        "iteration_count": 0,
        "is_success": False,
        "test_output": ""
    }
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 25}

    # üî• ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô: ‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô debug_code ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
    async def event_generator():
        final_result = {} # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏ó‡∏≥ Summary
        
        try:
            # 2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å .invoke ‡πÄ‡∏õ‡πá‡∏ô .astream ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
            async for event in app.astream(initial_state, config=config):
                for node_name, output in event.items():
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
                    final_result.update(output)
                    
                    # 3. ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∑‡πà‡∏≠ Node ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà UI ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    data = {
                        "node": node_name,
                        "iteration": output.get("iteration_count", 0),
                        "is_success": output.get("is_success", False),
                        "test_output": output.get("test_output", "")
                    }
                    yield f"data: {json.dumps(data)}\n\n"
                    await asyncio.sleep(0.1) # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

            # 4. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Stream ‡∏à‡∏ö) ‡∏Ñ‡πà‡∏≠‡∏¢‡∏ó‡∏≥‡∏™‡πà‡∏ß‡∏ô Summary ‡πÅ‡∏•‡∏∞ Save Knowledge
            is_success = final_result.get('is_success', False)
            fixed_code = final_result.get('code_base', '')
            
            summary_text = "Analysis complete."
            if is_success and summarizer_llm:
                # ‡∏ó‡∏≥ Summary ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô
                summary_prompt = f"Explain this fix briefly: {fixed_code}"
                ai_msg = summarizer_llm.invoke([HumanMessage(content=summary_prompt)])
                summary_text = ai_msg.content
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏•‡∏á Vector Store
                print("üíæ Saving knowledge...")
                save_bug_report(request.error, request.code, fixed_code, summary_text)

            # 5. ‡∏™‡πà‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Completed ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            final_data = {
                "status": "completed",
                "fixed_code": fixed_code,
                "summary": summary_text,
                "is_success": is_success,
                "knowledge": similar_cases,
                "test_output": final_result.get("test_output", "")
            }
            yield f"data: {json.dumps(final_data)}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    # 6. ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô StreamingResponse
    return StreamingResponse(event_generator(), media_type="text/event-stream")
            
   

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(api, host="0.0.0.0", port=8000)