# file: agents/reviewer.py

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import os
from dotenv import load_dotenv

load_dotenv()

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Developer)
if os.getenv("DEEPSEEK_API_KEY"):
    llm = ChatOpenAI(
        model="deepseek-coder", 
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
elif os.getenv("OPENAI_API_KEY"):
    llm = ChatOpenAI(
        model="gpt-3.5-turbo", 
        api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0
    )
else:
    raise ValueError("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env")

def reviewer_node(state):
    """
    Reviewer Agent: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á (Polyglot Version)
    """
    print("--- SENIOR REVIEWER IS WORKING ---")
    
    current_code = state['code_base']
    error_context = state['error_context']
    test_output = state.get('test_output', '') # üî• ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Docker ‡∏°‡∏≤‡∏î‡∏π‡∏î‡πâ‡∏ß‡∏¢

    # 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î Prompt ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Polyglot (‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏©‡∏≤)
    system_prompt = """You are a Senior Polyglot Code Reviewer. 
    
    CRITERIA FOR APPROVAL:
    1. Correctness: Does the code fix the specific error?
    2. Execution: Does the code run successfully in Docker?
    3. EFFICIENCY: Is the time complexity optimal? (Reject O(n^2) if O(n) is possible).
    4. Style: Is the code clean and strictly typed?
    
    OUTPUT FORMAT:
    - If perfect: "APPROVE"
    - If logic works but slow: "FEEDBACK: The solution is correct but inefficient. Please optimize using [suggested method]..."
    - If fails: "FEEDBACK: [Reason]..."
    """
    
    user_content = f"""
    ### ERROR TO FIX:
    {error_context}

    ### FIXED CODE:
    {current_code}

    ### EXECUTION LOGS (From Docker):
    {test_output}
    """

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_content)
    ]
    
    response = llm.invoke(messages)
    content = response.content.strip()
    
    print(f"--- REVIEW RESULT: {content[:100]}... ---")

    # 2. ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    is_passed = "APPROVE" in content.upper()
    
    return {
        "reflection_logs": [content],
        "is_success": is_passed  # üî• ‡∏™‡πà‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏µ‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Graph ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏´‡∏¢‡∏∏‡∏î‡∏ß‡∏¥‡πà‡∏á
    }